{"cells":[{"metadata":{"_uuid":"313feaa23cb3ab0d865d51a61695574788956a9f"},"cell_type":"markdown","source":"# Introduction\nMachine learning competitions are a great way to improve your data science skills and measure your progress. \n\nIn this exercise, you will create and submit predictions for a Kaggle competition. You can then improve your model (e.g. by adding features) to improve and see how you stack up to others taking this course.\n\nThe steps in this notebook are:\n1. Build a Random Forest model with all of your data (**X** and **y**)\n2. Read in the \"test\" data, which doesn't include values for the target.  Predict home values in the test data with your Random Forest model.\n3. Submit those predictions to the competition and see your score.\n4. Optionally, come back to see if you can improve your model by adding features or changing your model. Then you can resubmit to see how that stacks up on the competition leaderboard."},{"metadata":{"_uuid":"fd406db07127c0a324b7d62d19083342946933fb"},"cell_type":"markdown","source":"## Recap\nHere's the code you've written so far. Start by running it again."},{"metadata":{"_uuid":"4c5250be76ec9c271f987ef136d15d560515c5de","trusted":true},"cell_type":"code","source":"# Code you have previously used to load data\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\n\n#from xgboost import XGBRegressor\nfrom xgboost.sklearn import XGBRegressor\n\n\nimport xgboost as xgb\nimport lightgbm as lgb\n\n\n#from sklearn.impute import SimpleImputer\nimport numpy as np\nfrom scipy.stats import skew\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n\n\n\n\n# Path of the file to read. We changed the directory structure to simplify submitting to a competition\niowa_file_path = '../input/train.csv'\ntest_data_path = '../input/test.csv'\n\ntrain_data = pd.read_csv(iowa_file_path)\ntest_data = pd.read_csv(test_data_path)\n\n# Create target object and call it y\n#y = train_data.SalePrice\n\n#train_data[\"OverallQual-s2\"] = train_data[\"OverallQual\"] ** 2\n#train_data[\"OverallQual-s3\"] = train_data[\"OverallQual\"] ** 3\n#train_data[\"OverallQual-Sq\"] = np.sqrt(train_data[\"OverallQual\"])\n\n#test_data[\"OverallQual-s2\"] = test_data[\"OverallQual\"] ** 2\n#test_data[\"OverallQual-s3\"] = test_data[\"OverallQual\"] ** 3\n#test_data[\"OverallQual-Sq\"] = np.sqrt(test_data[\"OverallQual\"])\n\n# remove 2 obnormal points\ntrain_data = train_data.drop(train_data[train_data['Id'] == 1299].index)\ntrain_data = train_data.drop(train_data[train_data['Id'] == 524].index)\n# drop Id column\n#train_data.drop(\"Id\", axis = 1, inplace = True)\n#test_data.drop(\"Id\", axis = 1, inplace = True)\n\n#y = train_data.SalePrice\n#y = np.log1p(y)\n#print (y.head())\n\n#candidate_X_predictors = train_data.drop(['Id', 'SalePrice'] + cols_with_missing, axis=1)\n#candidate_test_predictors = test_data.drop(['Id'] + cols_with_missing, axis=1)\n\n\n#total = train_data.isnull().sum().sort_values(ascending=False)\n#percent = (train_data.isnull().sum()/train_data.isnull().count()).sort_values(ascending=False)\n#missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n\n#train_data = train_data.drop((missing_data[missing_data['Total'] > 1]).index,1)\n#candidate_X_predictors = train_data.drop(train_data.loc[train_data['Electrical'].isnull()].index)\n#candidate_X_predictors = candidate_X_predictors.drop(pd.Int64Index([1379], dtype='int64'))\n#test_data = test_data.drop((missing_data[missing_data['Total'] > 1]).index,1)\n#y = y.drop(pd.Int64Index([1379], dtype='int64'))\n#y = candidate_X_predictors.SalePrice\n#candidate_X_predictors = candidate_X_predictors.drop(['Id', 'SalePrice'], axis=1)\n#candidate_test_predictors = test_data.drop(['Id'], axis=1)\n\n#low_cardinality_cols = [cname for cname in candidate_X_predictors.columns if \n#                                candidate_X_predictors[cname].nunique() < 10 and\n#                                candidate_X_predictors[cname].dtype == \"object\"]\n#numeric_cols = [cname for cname in candidate_X_predictors.columns if \n#                                candidate_X_predictors[cname].dtype in ['int64', 'float64']]\n#my_cols = low_cardinality_cols + numeric_cols\n#train_predictors = candidate_X_predictors[my_cols]\n#test_predictors = candidate_test_predictors[my_cols]\n\n#print (train_predictors.shape, test_predictors.shape, y.shape)\n\n#cols_with_missing = [col for col in train_predictors.columns \n#                                 if train_predictors[col].isnull().any()]  \n\n#print (cols_with_missing)\n#imputed_X_train_plus = train_predictors.copy()\n#imputed_X_test_plus = test_predictors.copy()\n\n#for col in cols_with_missing:\n#    imputed_X_train_plus[col + '_was_missing'] = imputed_X_train_plus[col].isnull()\n#    imputed_X_test_plus[col + '_was_missing'] = imputed_X_test_plus[col].isnull()\n    \n#train_predictors = imputed_X_train_plus\n#test_predictors = imputed_X_test_plus\n\n#since test set has some columns which is dropped from train set, so we need to fill in missing values to align with train set\n#print (test_predictors.describe(), test_predictors.shape[1])\n#my_imputer = SimpleImputer(strategy='most_frequent')\n#my_imputer2 = SimpleImputer()\n#imputed_train = pd.DataFrame(my_imputer.fit_transform(train_predictors.select_dtypes(include = ['O']))) # for object\n#imputed_test = pd.DataFrame(my_imputer.fit_transform(test_predictors.select_dtypes(include = ['O']))) # for object\n#imputed_train2 = pd.DataFrame(my_imputer2.fit_transform(train_predictors.select_dtypes(include = ['int64', 'float64']))) # for number\n#imputed_test2 = pd.DataFrame(my_imputer2.fit_transform(test_predictors.select_dtypes(include = ['int64', 'float64']))) # for number\n#imputed_train.columns = train_predictors.select_dtypes(include = ['O']).columns\n#imputed_test.columns = test_predictors.select_dtypes(include = ['O']).columns\n#imputed_train2.columns = train_predictors.select_dtypes(include = ['int64', 'float64']).columns\n#imputed_test2.columns = test_predictors.select_dtypes(include = ['int64', 'float64']).columns\n\n#tmp_obj = train_predictors.copy()\n#for col in imputed_train.columns:\n#    tmp_obj[col] = imputed_train[col]\n#for col in imputed_test2.columns:\n#    tmp_obj[col] = imputed_train2[col]\n#train_predictors = tmp_obj\n\n#tmp_obj = test_predictors.copy()\n#for col in imputed_test.columns:\n#    tmp_obj[col] = imputed_test[col]\n#for col in imputed_test2.columns:\n#    tmp_obj[col] = imputed_test2[col]\n#test_predictors = tmp_obj\n\n#skewness = train_predictors.select_dtypes(include = ['int64', 'float64']).apply(lambda x: skew(x))\n#skewness = skewness[abs(skewness) > 0.5]\n#print(str(skewness.shape[0]) + \" skewed numerical features to log transform\")\n#skewed_features = skewness.index\n#train_predictors[skewed_features] = np.log1p(train_predictors[skewed_features])\n\n#skewness = test_predictors.select_dtypes(include = ['int64', 'float64']).apply(lambda x: skew(x))\n#skewness = skewness[abs(skewness) > 0.5]\n#print(str(skewness.shape[0]) + \" skewed numerical features to log transform\")\n#skewed_features = skewness.index\n#test_predictors[skewed_features] = np.log1p(test_predictors[skewed_features])\n\n#numerical_features = train_predictors.select_dtypes(exclude = [\"object\"]).columns\n\n\n#one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)\n#one_hot_encoded_test_predictors = pd.get_dummies(test_predictors)\n\n#print (\"Cond2 \", one_hot_encoded_training_predictors['Condition2_Norm'])\n#cols_with_missing = [col for col in one_hot_encoded_test_predictors.columns \n#                                 if col.startswith(\"Condition2\")] \n\n\n# i found join='inner' outperform join='left' option\n#X, final_test = one_hot_encoded_training_predictors.align(one_hot_encoded_test_predictors,\n#                                                join='inner', \n#                                                axis=1)\n\n# following code is only meaningful for join='left' option\n#my_imputer3 = SimpleImputer(strategy='constant', fill_value=0)\n#final_test = pd.DataFrame(my_imputer3.fit_transform(final_test))\n#final_test.columns = X.columns\n\n\n#cols_with_missing = [col for col in final_test.columns \n#                                 if final_test[col].isnull().any()] \n#print (cols_with_missing)\n\n#print (one_hot_encoded_training_predictors.describe())\n#print (one_hot_encoded_test_predictors.describe())\n#print (X.describe())\n#print (final_test.describe())\n\nall_data = pd.concat((train_data.loc[:,'MSSubClass':'SaleCondition'],\n                      test_data.loc[:,'MSSubClass':'SaleCondition']))\nprint (all_data.shape)\n\ntrain_data[\"SalePrice\"] = np.log1p(train_data[\"SalePrice\"])\n\n# Feature engineering (Clean up)\nall_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\nall_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\nall_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\nall_data[\"GarageType\"] = all_data[\"GarageType\"].fillna(\"None\")\nall_data[\"GarageFinish\"] = all_data[\"GarageFinish\"].fillna(\"None\")\nall_data[\"GarageQual\"] = all_data[\"GarageQual\"].fillna(\"None\")\nall_data[\"GarageCond\"] = all_data[\"GarageCond\"].fillna(\"None\")\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')\nall_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\nall_data = all_data.drop(['Utilities'], axis=1)\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\nall_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")\n\nall_data_na = (all_data.isnull().sum() / len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nprint (missing_data.head())\n\n# Feature engineering (Transform)\n#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)\n\nall_data['YearBuilt'] = all_data['YearBuilt'].astype(str)\nall_data['YearRemodAdd'] = all_data['YearRemodAdd'].astype(str)\n#all_data['GarageYrBlt'] = all_data['GarageYrBlt'].astype(str) #seems column with NaN is treated as float number\n\n\nfrom sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold', 'YearBuilt', 'YearRemodAdd')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape \nprint('Shape all_data: {}'.format(all_data.shape))\n\n\n# Adding total sqfootage feature \nall_data['TotalSF'] = pd.Series(all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF'])\n\n\nnumeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\nskewed_feats = skewed_feats[abs(skewed_feats) > 0.5]\nskewed_feats = skewed_feats.index\n\n#use box cox instead of log1\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewed_feats.shape[0]))\nfrom scipy.special import boxcox1p\nlam = 0.15\nfor feat in skewed_feats:\n    #all_data[feat] += 1\n    all_data[feat] = boxcox1p(all_data[feat], lam)\n#all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n\nall_data = pd.get_dummies(all_data)\nall_data = all_data.fillna(all_data.mean())\n\n# make sure there is no missing data\nall_data_na = (all_data.isnull().sum() / len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nprint (missing_data.head())\n\n\nX_train = all_data[:train_data.shape[0]]\nX_test = all_data[train_data.shape[0]:]\ny = train_data.SalePrice\n\n# try to find outliers\n#import statsmodels.api as sm\n\n#ols = sm.OLS(endog = y, exog = X_train)\n#fit = ols.fit()\n#test2 = fit.outlier_test()['bonf(p)']\n#outliers = list(test2[test2<1e-3].index) \noutliers = [462, 632, 1324, 1370, 1453]\nprint (outliers)\n\nX_train = X_train.drop(X_train.index[outliers])\ny = y.drop(y.index[outliers])\n\n# find big mean columns and then normalize them with standard deviation\nbig_col = (X_train.mean() > 100).index\ntmp_mean = X_train[big_col].mean().sort_values(ascending=False)\nprint (tmp_mean.head(5))\n\n\nstdSc = StandardScaler()\nX_train.loc[:, [\"GarageArea\"]] = stdSc.fit_transform(X_train.loc[:, [\"GarageArea\"]])\nX_test.loc[:, [\"GarageArea\"]] = stdSc.transform(X_test.loc[:, [\"GarageArea\"]])\n\n\nfrom sklearn.linear_model import Ridge, RidgeCV, ElasticNet, ElasticNetCV, Lasso, LassoCV, LassoLarsCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.kernel_ridge import KernelRidge\n\n\n#def rmse_cv(model):\n#    rmse= np.sqrt(-cross_val_score(model, X_train, y, scoring=\"neg_mean_squared_error\", cv = 5))\n#    return(rmse)\n\ndef rmse_cv(model):\n    kf = KFold(5, shuffle=True, random_state=42).get_n_splits(X_train.values)\n    rmse= np.sqrt(-cross_val_score(model, X_train.values, y.values, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n\n# Lasso - L1\nmodel_lasso = LassoCV(alphas = [0.0001, 0.0003, 0.0005, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, \n                          0.3, 0.6, 1], \n                max_iter = 50000, cv = 5)\nmodel_lasso.fit(X_train, y)\nalpha = model_lasso.alpha_\nprint(\"Best alpha :\", alpha)\n\nprint(\"Try again for more precision with alphas centered around \" + str(alpha))\nmodel_lasso = LassoCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, \n                          alpha * .85, alpha * .9, alpha * .95, alpha, alpha * 1.05, \n                          alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3, alpha * 1.35, \n                          alpha * 1.4], \n                max_iter = 50000, cv = 5)\nmodel_lasso.fit(X_train, y)\nalpha = model_lasso.alpha_\nprint(\"Best alpha :\", alpha)\n\n#coef = pd.Series(model_lasso.coef_, index = X_train.columns)\n#print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\n\n#print(\"Lasso RMSE on Training set : \", rmse_cv(model_lasso).mean())\n# make more robust to outliers\np_lasso = make_pipeline(RobustScaler(), Lasso(alpha =alpha))\nprint (\"Lasso RMSE on Training set with RobustScaler : \", rmse_cv(p_lasso).mean())\n\n# Ridge - L2\nridge = RidgeCV(alphas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 20, 40, 60])\nridge.fit(X_train, y)\nalpha = ridge.alpha_\nprint(\"Best alpha :\", alpha)\n\nprint(\"Try again for more precision with alphas centered around \" + str(alpha))\nridge = RidgeCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, \n                          alpha * .9, alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15,\n                          alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4, alpha * 1.5], \n                cv = 5)\nridge.fit(X_train, y)\nalpha = ridge.alpha_\nprint(\"Best alpha :\", alpha)\n\n#print(\"Ridge RMSE on Training set :\", rmse_cv(ridge).mean())\n#y_train_rdg = ridge.predict(X_train)\np_ridge = make_pipeline(RobustScaler(), Ridge(alpha=alpha))\nprint(\"Ridge RMSE on Training set with RobustScaler :\", rmse_cv(p_ridge).mean())\n\n\n# ElasticNet - L1 & L2\nelasticNet = ElasticNetCV(l1_ratio = [0.1, 0.3, 0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95, 1],\n                          alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, \n                                    0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6], \n                          max_iter = 50000, cv = 5)\nelasticNet.fit(X_train, y)\nalpha = elasticNet.alpha_\nratio = elasticNet.l1_ratio_\nprint(\"Best l1_ratio :\", ratio)\nprint(\"Best alpha :\", alpha )\n\nprint(\"Try again for more precision with l1_ratio centered around \" + str(ratio))\nelasticNet = ElasticNetCV(l1_ratio = [ratio * .85, ratio * .9, ratio * .95, ratio, ratio * 1.05, ratio * 1.1, ratio * 1.15],\n                          alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6], \n                          max_iter = 50000, cv = 5)\nelasticNet.fit(X_train, y)\nif (elasticNet.l1_ratio_ > 1):\n    elasticNet.l1_ratio_ = 1    \nalpha = elasticNet.alpha_\nratio = elasticNet.l1_ratio_\nprint(\"Best l1_ratio :\", ratio)\nprint(\"Best alpha :\", alpha )\n\nprint(\"Now try again for more precision on alpha, with l1_ratio fixed at \" + str(ratio) + \n      \" and alpha centered around \" + str(alpha))\nelasticNet = ElasticNetCV(l1_ratio = ratio,\n                          alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, alpha * .9, \n                                    alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3, \n                                    alpha * 1.35, alpha * 1.4], \n                          max_iter = 50000, cv = 5)\nelasticNet.fit(X_train, y)\nif (elasticNet.l1_ratio_ > 1):\n    elasticNet.l1_ratio_ = 1    \nalpha = elasticNet.alpha_\nratio = elasticNet.l1_ratio_\nprint(\"Best l1_ratio :\", ratio)\nprint(\"Best alpha :\", alpha )\n\nprint(\"ElasticNet RMSE on Training set :\", rmse_cv(elasticNet).mean())\n#p_ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=alpha, l1_ratio=ratio, random_state=3))\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=alpha, l1_ratio=ratio, random_state=3))\nprint(\"ElasticNet RMSE on Training set with RobustScaler:\", rmse_cv(ENet).mean())\n\n\n# KRR\nKRR = KernelRidge(alpha=0.84, kernel='polynomial', degree=2, coef0=2.5)\nprint(\"KernelRidge RMSE on Training set :\", rmse_cv(KRR).mean())\n\n# Split into validation and training data\n#train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n\n#stdSc = StandardScaler()\n#train_X.loc[:, numerical_features] = stdSc.fit_transform(train_X.loc[:, numerical_features])\n#val_X.loc[:, numerical_features] = stdSc.transform(val_X.loc[:, numerical_features])\n#X.loc[:, numerical_features] = stdSc.transform(X.loc[:, numerical_features])\n#final_test.loc[:, numerical_features] = stdSc.transform(final_test.loc[:, numerical_features])\n\n\n# Specify Model\n#iowa_model = DecisionTreeRegressor(random_state=1)\n# Fit Model\n#iowa_model.fit(train_X, train_y)\n\n# Make validation predictions and calculate mean absolute error\n#val_predictions = iowa_model.predict(val_X)\n#val_mae = mean_absolute_error(val_predictions, val_y)\n#print(\"Validation MAE when not specifying max_leaf_nodes: {:,.4f}\".format(val_mae))\n\n# Using best value for max_leaf_nodes\n#iowa_model = DecisionTreeRegressor(max_leaf_nodes=100, random_state=1)\n#iowa_model.fit(train_X, train_y)\n#val_predictions = iowa_model.predict(val_X)\n#val_mae = mean_absolute_error(val_predictions, val_y)\n#print(\"Validation MAE for best value of max_leaf_nodes: {:,.4f}\".format(val_mae))\n\n# Define the model. Set random_state to 1\n#rf_model = RandomForestRegressor(random_state=1)\n#rf_model.fit(X_train, y)\n#rf_val_predictions = rf_model.predict(val_X)\n#rf_val_mae = mean_absolute_error(rf_val_predictions, val_y)\n\n#print(\"Validation MAE for Random Forest Model: {:,.4f}\".format(rmse_cv(rf_model).mean()))\n\n# add XGBoost algo\n#xgb_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, random_state=1)\nxgb_model = xgb.XGBRegressor(\n learning_rate =0.01,\n n_estimators=2407,\n max_depth=3,\n min_child_weight=3,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n reg_alpha = 1.5e-05,\n objective= 'reg:linear',\n nthread=4,\n scale_pos_weight=1,\n seed=27)\n#xgb_model.fit(X_train, y, early_stopping_rounds=5, eval_set=[(val_X, val_y)], verbose=False)\n#print(\"XGBRegressor RMSE on Training set :\", rmse_cv(xgb_model).mean())\n#xgb_val_predictions = xgb_model.predict(val_X)\n#xgb_val_mae = mean_absolute_error(np.expm1(xgb_val_predictions), np.expm1(val_y))\n#print(\"Validation MAE for XGBoot Model: {:,.5f}\".format(xgb_val_mae))\n\n\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n\n#dtrain = xgb.DMatrix(X_train, label = y)\n#dtest = xgb.DMatrix(X_test)\n#params = {\"max_depth\":2, \"eta\":0.1}\n#model = xgb.cv(params, dtrain,  num_boost_round=500, early_stopping_rounds=100)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9608de9bbb898e9ab7d77f2e9400a1c73ec4d3e2"},"cell_type":"markdown","source":"# Creating a Model For the Competition\n\nBuild a Random Forest model and train it on all of **X** and **y**.  "},{"metadata":{"_uuid":"5e26a140e0dfe8fe6d8e3ae4f6b64b382c812b44","trusted":true},"cell_type":"code","source":"# To improve accuracy, create a new Random Forest model which you will train on all training data\n#rf_model_on_full_data = RandomForestRegressor(random_state=1)\n#xgb_model_on_full_data = XGBRegressor(n_estimators=1000, learning_rate=0.05, random_state=1)\n\n# fit rf_model_on_full_data on all data from the \n#rf_model_on_full_data.fit(X, y)\n#xgb_model_on_full_data.fit(X, y, early_stopping_rounds=5, eval_set=[(val_X, val_y)], verbose=False)\n\n\n#model_xgb = xgb.XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1) #the params were tuned using xgb.cv\n#model_xgb = XGBRegressor(n_estimators=1000, max_depth=2, learning_rate=0.05) #the params were tuned using xgb.cv\n\n\nclass StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)\n\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\n\n\nstacked_averaged_models = StackingAveragedModels(base_models = (ENet, p_ridge, KRR),\n                                                 meta_model = p_lasso)\nprint (\"Evaluating ...\")\n#score = rmse_cv(stacked_averaged_models)\n#print(\"Stacking Averaged models score: \", score.mean())\nstacked_averaged_models.fit(X_train.values, y.values)\nstacked_train_pred = stacked_averaged_models.predict(X_train.values)\nstacked_preds = np.expm1(stacked_averaged_models.predict(X_test.values))\nprint(rmsle(y, stacked_train_pred))\n\nmodel_lgb.fit(X_train.values, y.values)\nlgb_train_pred = model_lgb.predict(X_train.values)\nlgb_pred = np.expm1(model_lgb.predict(X_test.values))\nprint(rmsle(y, lgb_train_pred))\n\n\nprint (\"Starting fit ...\")\n#p_lasso.fit(X_train, y)\nxgb_model.fit(X_train, y)\n#stacked_averaged_models.fit(X_train.values, y.values)\n\n#print (rmse_cv(model_lasso).mean())\n#print (rmse_cv(xgb_model).mean())\n\n#predictions = pd.DataFrame({\"xgb\":xgb_preds, \"lasso\":lasso_preds})\n#predictions.plot(x = \"xgb\", y = \"lasso\", kind = \"scatter\")\n\n#lasso_preds = np.expm1(p_lasso.predict(X_test))\n#elasticNet_preds = np.expm1(elasticNet.predict(X_test))\nxgb_preds = np.expm1(xgb_model.predict(X_test))\n\n\npreds = 0.1*lgb_pred + 0.2*xgb_preds + 0.7*stacked_preds\n#preds = stacked_preds\n\nsolution = pd.DataFrame({\"id\":test_data.Id, \"SalePrice\":preds})\nsolution.to_csv(\"ridge_sol.csv\", index = False)\nprint (\"Done\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f9ccc6fba3c82016f9dd6a27710358bbf86b024"},"cell_type":"markdown","source":"# Make Predictions\nRead the file of \"test\" data. And apply your model to make predictions"},{"metadata":{"_uuid":"28e61db3881391dfc053a2ee87d3b77913e2fdcb","trusted":true},"cell_type":"code","source":"#from sklearn.impute import SimpleImputer\n#import numpy as np\n\ntest_X = final_test\n# create test_X which comes from test_data but includes only the columns you used for prediction.\n# The list of columns is stored in a variable called features\n#test_X = test_data[features]\nprint (test_X.describe())\n\n# make predictions which we will submit. \n#test_preds = rf_model_on_full_data.predict(test_X)\ntest_preds = xgb_model_on_full_data.predict(test_X)\ntest_preds = np.expm1(test_preds)\n\n# The lines below shows you how to save your data in the format needed to score it in the competition\noutput = pd.DataFrame({'Id': test_data.Id,\n                       'SalePrice': test_preds})\n\noutput.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6844e310c9890f9af4f364c2aa831594e5b6586"},"cell_type":"markdown","source":"# Test Your Work\nAfter filling in the code above:\n1. Click the **Commit and Run** button. \n2. After your code has finished running, click the small double brackets **<<** in the upper left of your screen.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n3. Go to the output tab at top of your screen. Select the button to submit your file to the competition.  \n4. If you want to keep working to improve your model, select the edit button. Then you can change your model and repeat the process.\n\nCongratulations, you've started competing in Machine Learning competitions.\n\n# Continuing Your Progress\nThere are many ways to improve your model, and **experimenting is a great way to learn at this point.**\n\nThe best way to improve your model is to add features.  Look at the list of columns and think about what might affect home prices.  Some features will cause errors because of issues like missing values or non-numeric data types. \n\nLevel 2 of this course will teach you how to handle these types of features. You will also learn to use **xgboost**, a technique giving even better accuracy than Random Forest.\n\n\n# Other Courses\nThe **[Pandas course](https://kaggle.com/Learn/Pandas)** will give you the data manipulation skills to quickly go from conceptual idea to implementation in your data science projects. \n\nYou are also ready for the **[Deep Learning](https://kaggle.com/Learn/Deep-Learning)** course, where you will build models with better-than-human level performance at computer vision tasks.\n\n---\n**[Course Home Page](https://www.kaggle.com/learn/machine-learning)**\n\n**[Learn Discussion Forum](https://kaggle.com/learn-forum)**.\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}